#if HZ3_S121_E_CADENCE_COLLECT

// Global page list: g_owner_pages[owner][sc]
static Hz3OwnerPageListGlobal g_owner_pages[HZ3_NUM_SHARDS][HZ3_SMALL_NUM_SC];

// S121-E stats
#if HZ3_S121_E_STATS
static _Atomic uint64_t g_s121e_page_alloc = 0;      // pages added to owner list
static _Atomic uint64_t g_s121e_scan_pages = 0;      // pages scanned in pop
static _Atomic uint64_t g_s121e_page_drain = 0;      // pages with remote_head drained
static _Atomic uint64_t g_s121e_pop_objs = 0;        // objects returned by pop
static _Atomic uint64_t g_s121e_chunk_alloc = 0;     // chunks allocated
static _Atomic uint64_t g_s121e_push_delayed = 0;    // Phase 8: pages marked DELAYED
static _Atomic uint64_t g_s121e_skip_normal = 0;     // Phase 8: pages skipped (NORMAL)

#define S121E_STAT_INC(var) atomic_fetch_add_explicit(&(var), 1, memory_order_relaxed)
#define S121E_STAT_ADD(var, n) atomic_fetch_add_explicit(&(var), (n), memory_order_relaxed)
#else
#define S121E_STAT_INC(var) ((void)0)
#define S121E_STAT_ADD(var, n) ((void)0)
#endif

// -----------------------------------------------------------------------------
// Chunk allocation (append-only, never freed)
// -----------------------------------------------------------------------------
static inline Hz3PageChunk* hz3_pagechunk_new(void) {
    Hz3PageChunk* c = (Hz3PageChunk*)mmap(NULL, sizeof(Hz3PageChunk),
                                          PROT_READ | PROT_WRITE,
                                          MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
    if (c == MAP_FAILED) return NULL;
    atomic_store_explicit(&c->used, 0, memory_order_relaxed);
    c->next = NULL;
    S121E_STAT_INC(g_s121e_chunk_alloc);
    return c;
}

// -----------------------------------------------------------------------------
// Append page to global list (called at page allocation only)
// -----------------------------------------------------------------------------
void hz3_owner_pagelist_append(int sc, void* page) {
    // Get owner from page header
    Hz3S121PageHdr* hdr = (Hz3S121PageHdr*)page;
    uint32_t owner = hdr->owner;
    Hz3OwnerPageListGlobal* L = &g_owner_pages[owner][sc];

    // Get or initialize tail
    Hz3PageChunk* tail = atomic_load_explicit(&L->tail, memory_order_acquire);
    if (tail == NULL) {
        Hz3PageChunk* c = hz3_pagechunk_new();
        if (!c) return;  // OOM - skip this page
        Hz3PageChunk* expected = NULL;
        if (atomic_compare_exchange_strong_explicit(&L->head, &expected, c,
                memory_order_release, memory_order_relaxed)) {
            atomic_store_explicit(&L->tail, c, memory_order_release);
            tail = c;
        } else {
            // Someone else initialized - free our chunk and use theirs
            munmap(c, sizeof(Hz3PageChunk));
            tail = atomic_load_explicit(&L->tail, memory_order_acquire);
            if (!tail) tail = atomic_load_explicit(&L->head, memory_order_acquire);
        }
    }

    // Append to tail chunk (or create new chunk if full)
    for (;;) {
        uint32_t i = atomic_fetch_add_explicit(&tail->used, 1, memory_order_acq_rel);
        if (i < HZ3_S121E_CHUNK_CAP) {
            tail->pages[i] = page;
            atomic_fetch_add_explicit(&L->page_count, 1, memory_order_relaxed);
            S121E_STAT_INC(g_s121e_page_alloc);
            return;
        }
        // Chunk is full - need a new one
        Hz3PageChunk* next = tail->next;
        if (next == NULL) {
            Hz3PageChunk* c = hz3_pagechunk_new();
            if (!c) return;  // OOM
            // CAS to append new chunk
            if (__atomic_compare_exchange_n(&tail->next, &next, c,
                                           0, __ATOMIC_RELEASE, __ATOMIC_RELAXED)) {
                atomic_store_explicit(&L->tail, c, memory_order_release);
                tail = c;
            } else {
                munmap(c, sizeof(Hz3PageChunk));
                tail = next;  // Someone else added a chunk
            }
        } else {
            // Advance to existing next chunk
            atomic_store_explicit(&L->tail, next, memory_order_release);
            tail = next;
        }
    }
}

// -----------------------------------------------------------------------------
// Collect remote objects from pages (called in pop slow path)
// Returns number of objects collected
// -----------------------------------------------------------------------------
static inline int hz3_owner_pages_collect(uint32_t owner, uint32_t sc,
                                          int budget_pages, void** out, int want) {
    Hz3OwnerPageListGlobal* L = &g_owner_pages[owner][sc];

    Hz3PageChunk* head = atomic_load_explicit(&L->head, memory_order_acquire);
    if (!head) return 0;

    // Use cursor to distribute scan starting point across threads
    uint32_t start = atomic_fetch_add_explicit(&L->cursor, (uint32_t)budget_pages,
                                               memory_order_relaxed);

    // Find starting chunk (skip 'start' pages)
    Hz3PageChunk* c = head;
    uint32_t skip = start;
    while (c && skip > 0) {
        uint32_t used = atomic_load_explicit(&c->used, memory_order_acquire);
        if (used > HZ3_S121E_CHUNK_CAP) used = HZ3_S121E_CHUNK_CAP;
        if (skip < used) break;
        skip -= used;
        c = c->next;
    }
    // If we've wrapped around, start from head
    if (!c) {
        c = head;
        skip = 0;
    }

    int got = 0;
    int scanned = 0;

    // Scan pages
    for (; c && scanned < budget_pages && got < want; c = c->next) {
        uint32_t used = atomic_load_explicit(&c->used, memory_order_acquire);
        if (used > HZ3_S121E_CHUNK_CAP) used = HZ3_S121E_CHUNK_CAP;

        uint32_t start_idx = (c == head && skip < used) ? skip : 0;
        for (uint32_t i = start_idx; i < used && scanned < budget_pages && got < want; i++) {
            scanned++;
            void* page = c->pages[i];
            if (!page) continue;

            // Validate page (magic check - but NOT owner check since we're global now)
            Hz3S121PageHdr* hdr = (Hz3S121PageHdr*)page;
            if (hdr->magic != HZ3_PAGE_MAGIC) continue;

            // Phase 8: Skip NORMAL pages (remote_state == 0)
            // Only scan DELAYED pages (remote_state == 1)
            uint8_t state = atomic_load_explicit(&hdr->remote_state, memory_order_acquire);
            if (state == 0) {
                S121E_STAT_INC(g_s121e_skip_normal);
                continue;  // NORMAL - no remote objects, skip
            }

            // Check and drain remote_head (DELAYED page)
            if (atomic_load_explicit(&hdr->remote_head, memory_order_acquire) != NULL) {
                void* objs = atomic_exchange_explicit(&hdr->remote_head, NULL,
                                                      memory_order_acq_rel);
                if (objs) {
                    S121E_STAT_INC(g_s121e_page_drain);
                    // Collect objects into out[]
                    while (objs && got < want) {
                        out[got++] = objs;
                        objs = hz3_obj_get_next(objs);
                    }
                    // Return remaining to remote_head (push back)
                    if (objs) {
                        // Find tail and prepend to remote_head
                        void* tail = objs;
                        while (hz3_obj_get_next(tail)) {
                            tail = hz3_obj_get_next(tail);
                        }
                        void* old_head;
                        do {
                            old_head = atomic_load_explicit(&hdr->remote_head,
                                                            memory_order_relaxed);
                            hz3_obj_set_next(tail, old_head);
                        } while (!atomic_compare_exchange_weak_explicit(
                            &hdr->remote_head, &old_head, objs,
                            memory_order_release, memory_order_relaxed));
                        // Page still has objects, keep DELAYED
                    } else {
                        // Phase 8: Page completely drained, reset to NORMAL
                        atomic_store_explicit(&hdr->remote_state, 0, memory_order_release);
                    }
                }
            }
        }
        skip = 0;  // Only skip on first chunk
    }

    S121E_STAT_ADD(g_s121e_scan_pages, scanned);
    return got;
}

#if HZ3_S121_E_STATS
static _Atomic int g_s121e_stats_registered = 0;

static void hz3_s121e_stats_dump(void) {
    fprintf(stderr,
        "[S121-E stats] page_alloc=%lu scan_pages=%lu page_drain=%lu "
        "pop_objs=%lu chunk_alloc=%lu push_delayed=%lu skip_normal=%lu\n",
        (unsigned long)atomic_load(&g_s121e_page_alloc),
        (unsigned long)atomic_load(&g_s121e_scan_pages),
        (unsigned long)atomic_load(&g_s121e_page_drain),
        (unsigned long)atomic_load(&g_s121e_pop_objs),
        (unsigned long)atomic_load(&g_s121e_chunk_alloc),
        (unsigned long)atomic_load(&g_s121e_push_delayed),
        (unsigned long)atomic_load(&g_s121e_skip_normal));
}

static void hz3_s121e_stats_register_atexit(void) {
    if (atomic_exchange(&g_s121e_stats_registered, 1) == 0) {
        atexit(hz3_s121e_stats_dump);
    }
}
#define S121E_STAT_REGISTER() hz3_s121e_stats_register_atexit()
#else
#define S121E_STAT_REGISTER() ((void)0)
#endif

#endif  // HZ3_S121_E_CADENCE_COLLECT
